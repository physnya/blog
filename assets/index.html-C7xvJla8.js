import{_ as s,c as t,a as e,o as n}from"./app-CBHQ5mh_.js";const p={};function i(l,a){return n(),t("div",null,[...a[0]||(a[0]=[e('<div style="text-align:right;"><p>—— Qiufan LIN (深圳鹏城实验室)</p></div><div class="hint-container warning"><p class="hint-container-title">注意</p><p>本节课是一期邀请报告.</p></div><p>本节课的内容大量借鉴了 Yuan-Sun Ting 老师的一篇综述文章「Deep Learning in Astrophysics」.</p><h2 id="where-have-we-been-so-farr" tabindex="-1"><a class="header-anchor" href="#where-have-we-been-so-farr"><span>Where Have We Been So Farr?</span></a></h2><p>AI 这个概念最开始提出的时候并不是深度学习，而是想要创造一个系统来延展生物智能，做「人可以做的事情」. 当然现在讲的 AI 和以前讲的 AI 也是不一样的.</p><p>类别：</p><ul><li>连结主义：模仿神经网络结构，结构决定功能</li><li>符号主义：用抽象的符号来完成逻辑规则和知识的推理</li><li>行为主义：模仿人类和环境的交互</li></ul><p>两种学习的模式：Data-Driven (由数据驱动)，也就是我们不需要加入已有的知识，而是用 AI 来学习数据、预测数据；Knowledge-Driven (知识驱动)，先喂给 AI 人类已有的规则，但是问题在于知识不一定准确.</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>ANI (Narrow)</mtext><mo>⟶</mo><mtext>AGI (General)</mtext><mo>⟶</mo><mtext>ASI (super)</mtext></mrow><annotation encoding="application/x-tex">\\text{ANI (Narrow)}\\longrightarrow\\text{AGI (General)}\\longrightarrow\\text{ASI (super)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ANI (Narrow)</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟶</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">AGI (General)</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⟶</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ASI (super)</span></span></span></span></span></span></p><p>深度学习：很深的神经网络构建.</p><p>回到天文领域，我们的理论横跨了很多个尺度，观测手段有射电波段也有光学波段，这是一个量非常大的数据集. 这恰好适于用深度学习.</p><hr><p>深度学习的基本概念：首先是人工神经网络.</p><blockquote><p>单元分层，每一层之间用线连接，构成网络. 下一层处理的数据被整合传递到上一层的神经元.</p><p>「阈值逻辑」：接收到的信号在大于某个阈值的时候才被视为有这个信号.</p></blockquote><p>用模型预测结果和真实结果比较，同时考察结果的 bias，有几种情况：</p><ul><li>比较低的 variance、比较低的 bias：这是最好的情况</li><li>比较低的 variance、比较高的 bias：可以大致预测要确定的数据点在什么位置</li><li>比较高的 variance、比较低的 bias：可用，因为可以识别数据的集中情况</li><li>……</li></ul><p>有时候我们需要人为地加入一些 bias (inductive bias)，以此来优化结果.</p><blockquote><p>这个 inductive bias 实际上就是一种 prior.</p></blockquote><ul><li><p>CNN (Convolutional Neural Network)：不再进行全神经网络的连接，而是分成很多个 kernel，这样可以处理一些空间平移对称的数据，比如识别某个出现的物体.</p></li><li><p>RNN：与上面相对，时间平移对称，识别一些时序的数据.</p></li><li><p>Transformer：把整个数据排成一个张量，对这个张量做整体的分析，这样可以分析很长程的关联 (比如从张量的一个角到另一个角)，RNN 之类是学不到这个的.</p><p>这适合分析光谱数据.</p></li><li><p>GNN (Graph Neural Network)：数据分为不同节点和不同连接，当交换两个特定节点时 (比如三角形的两个顶点 A、B)，某些节点的数据性质应该不变 (比如三角形第三顶点 C).</p><p>这种学习适于分析随机数据.</p></li></ul><p>学习过程也有很多不同的类型，一般而言和要分析的数据相匹配.</p><ul><li><p>Supervised Learning (监督学习)：给出一个数据给整个网络，网络给出一个标签加在数据上.</p><p>比如图像识别，把不同的图像分类，输入手写数字然后返还数字是多少；或者输入星系图像，返还星系种类.</p></li><li><p>Unsupervised Learning (无监督学习)：不输出标签，而是在内部有一个 autoencoder，数据经过 autoencoder 就改变神经网络的一些结构，最后仍然返还数据本身.</p></li><li><p>Self-Supervised Learning (自监督学习)：</p><ol><li>Masked Autoencoder：擦除数据本身的一些特征，然后让 autoencoder 把数据还原出来.</li><li>Contrastive Learning：输入有一定相关性的数据，让模型把更相关的数据排到更近的位置，不相关的数据排到更远的位置.</li></ol></li></ul><p>上面是学习的类型，但是我们还没有了解生成式模型如何给出高维的数据：</p><ul><li><p>Variational Autoencoder (VAE)：首先在 Encoder 中对齐输出输入，然后 Decoder 负责在输出上面叠加一个变分分布，按照最大概率输出一个结果.</p><p>这个模式的硬伤在于，生成结果有很大概率失败，因为变分分布可能不准确.</p></li><li><p>Generative Adversarial Network (GAN)：对抗式学习，每一次生成两个结果然后判定哪一个更好，多次循环抉择出最好的结果.</p><p>这个模式也有硬伤，比如两个结果在某个方面可能都非常差，但是只要有一个方面更好就能胜出，最后的结果很有可能是有很大缺陷但是某一方面做得好的那一些.</p></li><li><p>Diffusion Model：业界目前用得最多的，每一次在数据上加噪声然后降噪.</p><p>硬伤在于，每次加噪声消耗大量的算力.</p></li><li><p>Normalizing Flow (归一化流)：学习时不断对某一区间做反射变换，之后逆变换回来.</p><p>缺点是反射变换只能作用于连续的数据类型，而且每次变换也消耗大量算力.</p></li></ul><p>下面我们说说神经网络能够为星系理论和宇宙学带来什么新的物理.</p><p><strong>Classification</strong>：比如用神经网络来判断星系类型.</p><p><strong>Parameter Estimation</strong>：参数估计，神经网络能够「看到」很多人类判别不出的内在数据关联，从而实现更好地预测参数.</p><p>/Example/</p><blockquote><p>把暗物质晕的模拟数据提供给图神经网络，神经网络通过对数据中星系的分布情况，给出暗物质的密度分布估计.</p></blockquote><p>上面这些是识别的一些结果，我们有时候能够用生成式的模型来完成一些事情. 比如模型可以根据学习到的星系图像来生成星系的图片，人眼确实无法分辨这些生成图像 —— 当然，实际上从物理的角度来对这些图做判断，模型生成的图片满足物理规律的程度并不很好.</p><p><strong>Domain Translation</strong>：域转移，生成式模型能够在跑了一次模拟之后帮助我们节约模拟的次数.</p><p><strong>Galaxy Deblending</strong>：很多时候星系在图像中重合在一起，我们不仅要拆分星系，还要把遮挡的部分补全出来 —— 这有别于传统的图像语义分割，传统语境下我们只需要对每一个像素做分类，但是在 Galaxy Deblending 的需求下我们要利用生成式的模型来补全星系. 目前生成的效果还是仅限于图像层面，在物理上仍然不够好.</p><p><strong>Anomaly Detection</strong>：异常检测.</p><p>刚才说的都是有关星系图像的应用，实际上还有很多对其他数据的分析，一个例子是 <strong>Spectrum Reconstruction</strong>，也就是光谱的分类和重建.</p><h2 id="where-are-we-going-next" tabindex="-1"><a class="header-anchor" href="#where-are-we-going-next"><span>Where Are We Going Next?</span></a></h2><p>前面一部分听起来好像是很 positive，相对地，后面我们将讲到各种挑战，这是相当 negative 的.</p><p>一些 risky 问题：</p><ul><li><p>Confidence Miscalibration (置信度误校准) —— very common</p><p>我们想用模型来做概率密度的预测 (也就是得到一个 PDF).</p><p>我们实际得到的 PDF 往往和真实数据的 PDF 不一致，计算机领域的很多人提出的观点是网络越深、置信度的误校准越大，而且高准确度对应低的置信度.</p><p>我们认为这个效应来自于 implicit model prior，也就是我们加入的条件和真实的条件概率是不一样的，最后通过 Bayes 公式算出来的条件概率就是错误的. 在这个意义上，有理由认为 Bayes 神经网络 (在节点之间的连接引入随机性，可以随机地断开或者连上一些连接) 也有置信度误校准的问题.</p><p>可能的解决方案：直接用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>-NN 来从模型生成的采样上算出 PDF，这里没有用到任何模型的推测. 至于如何确定最佳的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>，利用所谓的 Probability Integral Transform (PIT) 来进行判断：</p><blockquote><p>频率可以估计概率，所以如果把频率切分，应该是一个均匀的分布；要取的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> 就是使得局域频率分布最平直的那一个.</p></blockquote></li><li><p>Confusion between Association &amp; Causality —— very subtle</p><p>比如把一个湖面的图像给某个训练好的模型，模型可能会说这是一幅船的图片，因为它理解的只有「关联性」，没有真实的意义.</p><p>/Example/</p><blockquote><p>模型认为时空的几何构造不同会影响星系的性质. 但是这里完全搞反了因果关系，错误地把关联性理解为因果性.</p></blockquote><p>解决的思路是，不仅仅引入 X 和 Y 两个有关联的数据，同时还引入一个 Z 数据，这样就能把 X 和 Y 的性质独立开来：</p><ol><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>X</mtext><mo>→</mo><mtext>Z</mtext><mo>→</mo><mtext>Y</mtext></mrow><annotation encoding="application/x-tex">\\text{X}\\to\\text{Z}\\to\\text{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">X</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Z</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Y</span></span></span></span></span>：相当于多米诺骨牌，固定中间牌，前后就不相关.</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>X</mtext><mo>→</mo><mtext>Z</mtext><mo>←</mo><mtext>Y</mtext></mrow><annotation encoding="application/x-tex">\\text{X}\\to\\text{Z}\\leftarrow\\text{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">X</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Z</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Y</span></span></span></span></span>：相当于并联电路，只要按下开关 Z，X 和 Y 就不相关，都是导通的.</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>X</mtext><mo>←</mo><mtext>Z</mtext><mo>→</mo><mtext>Y</mtext></mrow><annotation encoding="application/x-tex">\\text{X}\\leftarrow\\text{Z}\\to\\text{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">X</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Z</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">Y</span></span></span></span></span>：相当于地面积水，地面积水可能来源于 X (下雨) 和 Y (洒水车)，两者不相关.</li></ol><p>通过这样的方式，Jin et al. (2025) 确认了星系中是否有大质量黑洞和星系的质量两者之间的因果关系.</p></li><li><p>Information Loss —— very intricate</p><p>用判别模型做了参数预测，发现传统模型的物理信息保留程度完全不足以助力当前的科学研究.</p><p>常规模型一般只识别低阶的统计性质，所以我们的解决方案是识别高阶性质，这需要对数据做联合概率分布建模.</p></li><li><p>Domain Shift —— very fundamental</p><p>例子是对手写数字做分类. 如果只有 0, 1, 2, 3, 4 几个数字，能够比较好地识别；但是给出「7」这个数据就完全不能识别，只能分类到 1 类别里面 —— 但是模型不会告诉你这个数据不可用，只会返回一个不对的结果，但是我们也不知道这个结果是否正确.</p><p>我们用模拟数据训练的模型在真实的数据上很有可能会遇到域迁移问题，因为我们不可能用所有数据去训练模型.</p><p>最简单的思路是做好模拟，减少系统效应；做好 simulation，保证抓住真实数据的那些关键的特质.</p></li></ul><p>后面的几个方向：</p><ul><li><p>Inject Physics into Neural Networks</p><p>直接在网络的优化过程中引入物理约束，让模型「懂物理」.</p><p>这里的问题在于，不一定所有的物理问题都可以被容纳到网络里面；即便融入了网络，我们得到的东西是近似还是精确的，也是一个不确定的事情.</p></li><li><p>Integrate Inductive and Deductive Logics</p><p>现有的天文学研究不一定能够被逻辑化到大模型内部.</p></li><li><p>Large Language Models &amp; Agents</p><p>Zhejiang Lab 开发的 AstroOne (天一大模型)，但是这仍然是一个通用型的模型.</p><p>AI Agent 是指以大语言模型作为「大脑」，调用专业领域工具的助手.</p></li><li><p>Foundation Models</p><p>基础模型 —— 真正的「大」模型，集成了大量的数据，但是仅适用于特定的任务，比如 AstroCLIP 适用于光谱图像分析.</p></li></ul><p>下一步我们还能做什么？</p><p>喂更多的数据、烧更多的算力，达成量变到质变？似乎更可行的方法是在已有的数据上加入新的技术，为我们的科研提供更多助力，暂时还没有到提供新知识构建的程度.</p><h2 id="q-a-环节" tabindex="-1"><a class="header-anchor" href="#q-a-环节"><span>Q &amp; A 环节</span></a></h2><ul><li><p>之前所说的「引入一个 Z 就能够判断 X 和 Y 之间的因果关系」是如何达成的？</p><p>首先我们必须要确定，这个 Z 在系统中的存在性，也就是对于一个对象至少应该有三个变量. 如果只有 X 和 Y，那么我们算概率的时候 X 和 Y 实际上是不独立的，算出来的东西有相关性.</p><p>但是引入 Z 之后，我们可以挑出所有同一个 Z 值的那些数据，然后再分析，这时候如果 X 和 Y 表现出相互独立的性质，那么 Z 就是一个中间的变量，它们之间的关系就像我们说的那几种；如果这时候 X 和 Y 仍然相互关联，那么它们之间就真的有关联性.</p></li><li><p>AI agent 的应用场景？</p><p>是一个提高效率的过程，它虽然不能够给出直接的答案，但是可以帮忙给一些资料.</p></li><li><p>如何把物理规律融入这个网络？</p><p>简单来说，以 N 体运动的规律为例，我们可以算 Hamiltonian，然后通过神经网络算出对应动量和坐标等等，对这些量也做出物理的约束，反推回去调节模型.</p></li></ul>',43)])])}const o=s(p,[["render",i]]),m=JSON.parse('{"path":"/astro-statistic/lesson-13-on-the-intersection-between-astronomy-and-ai/","title":"Lesson 13 On the Intersection between Astronomy and AI | 天体物理统计方法","lang":"zh-CN","frontmatter":{"title":"Lesson 13 On the Intersection between Astronomy and AI","permalink":"/astro-statistic/lesson-13-on-the-intersection-between-astronomy-and-ai/","createTime":"2025/12/11 21:51:14","description":"—— Qiufan LIN (深圳鹏城实验室) 注意 本节课是一期邀请报告. 本节课的内容大量借鉴了 Yuan-Sun Ting 老师的一篇综述文章「Deep Learning in Astrophysics」. Where Have We Been So Farr? AI 这个概念最开始提出的时候并不是深度学习，而是想要创造一个系统来延展生物智能，做...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Lesson 13 On the Intersection between Astronomy and AI\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-13T08:19:06.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://physnya.top/astro-statistic/lesson-13-on-the-intersection-between-astronomy-and-ai/"}],["meta",{"property":"og:site_name","content":"菲兹克斯喵"}],["meta",{"property":"og:title","content":"Lesson 13 On the Intersection between Astronomy and AI"}],["meta",{"property":"og:description","content":"—— Qiufan LIN (深圳鹏城实验室) 注意 本节课是一期邀请报告. 本节课的内容大量借鉴了 Yuan-Sun Ting 老师的一篇综述文章「Deep Learning in Astrophysics」. Where Have We Been So Farr? AI 这个概念最开始提出的时候并不是深度学习，而是想要创造一个系统来延展生物智能，做..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-13T08:19:06.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-13T08:19:06.000Z"}]]},"readingTime":{"minutes":11.36,"words":3407},"git":{"createdTime":1765613946000,"updatedTime":1765613946000,"contributors":[{"name":"physnya","username":"physnya","email":"676266673@qq.com","commits":1,"avatar":"https://avatars.githubusercontent.com/physnya?v=4","url":"https://github.com/physnya"}],"changelog":[{"hash":"7ec39d25474601539b3708cc1b315a1eac2380ff","time":1765613946000,"email":"676266673@qq.com","author":"physnya","message":"feat(note): add as note"}]},"autoDesc":true,"filePathRelative":"astro-statistic/lesson-13.md","headers":[]}');export{o as comp,m as data};
