---
changelog: false
title: 物理与深度学习
createTime: 2024/10/21 20:35:53
tags:
  - Physics
  - interdisciplinarity
  - lecture
permalink: /posts/dwn6rdfx/
---

——解析本年度 Nobel Prize for Physics & Chemistry.

这是物理系办的一个讲座，主要围绕今年的 Nobel 物理学奖颁发给深度学习的两位专家这个话题展开. 我在讲座上记录了一些要点，觉得很有启发，所以放在这里作为一个存档.

## 介绍

人工智能（Artificial Intelligence）$\supseteq$ 机器学习（Machine Learning）$\supseteq$ 深度学习（Deep Learning），这是一个包含的关系. 深度学习是这个领域的前沿，用模拟人类神经网络的方式实现人工智能.

最早在 1940 年代，提出数学模型；1958 年 Rosenblatt 感知器提出，是有隐藏层的前馈网络；日本科学家也在早期做出了很多奠基性工作.

1970 年代，这个领域进入一个寒冬，因为难以处理数量巨大的神经元. 解决这个问题的正是 John Hopfield ，今年的 Nobel Prize for Physics 得主. 他 1933 年出生，1969 得到巴克利奖（美国的一个凝聚态物理的奖项），之后想转到生物物理，但是后来发展出人工神经网络.

学过凝聚态物理的同学会知道，很多集体效应不是单个粒子的简单相加，而是呈现出非常特殊的统计性质. Hopfield 将这些思想应用到生物物理的领域中，他研究 DNA 的复制的试错，之后转向对联想记忆问题的研究.

> 联想记忆：从不完整或者有偏差的输入，还原记忆内容；这是区别于传统的计算机存储的.

Hopfield 考虑构建一个数学上的简单模型：Hopfield 网络，类似于 Ising Model ，神经元状态表征为$s_i=0,1$，链接权重为$w_{ij}=w_{ji}$，引入能量函数$E=-\frac{1}{2}\sum_{i\neq j}w_{ij}s_is_j$（这和 Ising Model 简直如出一辙！）

对这个网络做两件事情：

1. 训练：优化权重$w_{ij}$，用能量极小值的点<u>记录“记忆”</u>. $\to$ 赫布定律（1949）
2. 预测：固定$w_{ij}$，从输入的$s_i$出发，优化 Hopfield 能量，<u>寻找相似的记忆</u>. $\to$ 能量极小值对应“吸引子”，吸引子状态编码信息.

$\Longrightarrow$ 借助能量函数存储记忆，通过优化能量函数还原记忆.

这种方式被应用在早期的一些算法问题上，比如“行商问题”. 经过长期发展，现在的“现代 Hopfield 网络”已经发展到新的样式，技术内核已经更新，但是设计理念没有太大变化.

接下来， Hinton 等人对 Boltzmann 机的研究将这些知识向前更进一步：这里关注能量函数的<u>概率分布</u>，用 Monte Carlo 方法计算结果. Boltzmann 机除了权重$w_{ij}$以外还引入偏置$\theta_i$，将能量极小值的记忆存储改为马尔科夫链蒙特卡洛算法（MCMC）产生的概率分布存储记忆，依靠新的方法进行训练.

Hinton 发展受限 Boltzmann 机，同层神经元之间无连接，使得理论得到大幅深入. 2017 年，这种方法被用来求解量子多体问题.

Hinton 被称为“AI 教父”，坚持挺过了 AI 研究的两度寒冬，不仅自己做出很多贡献，同时还培养了一批知名的学生，其中甚至有 OpenAI 的主要技术贡献者. 他们发现，当神经网络的 Scale 一旦变大，很多之前无法解决的问题都能迎刃而解.

之后的 AI 发展从两个角度出发：架构更先进、应用更广泛，前者的著名例子是卷积网络（CNN）的图像处理，而后者最著名的则是 ChatGPT.

Nobel 委员会在介绍此次颁奖的成果时，提到这项成果对于天体物理（黑洞计算）和 Alpha-Fold 等各种领域都实现了广泛的应用.

## 和我们？

徐老师谈谈 AI 对他自己专业的帮助：从基于经验到基于数据. 人工智能非常适合处理这样的问题：高维的参数空间、明确的优化目标、丰富的训练数据或高效的数据产生器，当满足这些条件时，AI 就变得非常强大，Alpha-Fold 就是一个绝佳的例子. 传统的科学计算在数值模拟耗时非常大，而有深度学习驱动的科学计算可以实现高效而智能的科学计算.

徐老师做的是第一性原理计算，即“基于量子力学原理，计算现实模型”. 这几年发展出高效的深度学习新一代第一性原理计算方法，将重要的物理先验融入神经网络，“压缩” DFT 底层算法，（局域性原理：小体系演化至大体系，速度快；协变性原理：进一步增强泛化能力，举一反三，可扩展性强）. 这是一个全新的材料研究方式：MIND（Material INtelligence Database）.

推荐一些开源的学习资料：<a href="https://github.com/mzjb/">mzjb</a>（是徐老师的学生）有很多 repo ，如徐老师和学生共同做的 DeepH .

## 提问

- 神经网络发现物理规律的可解释性？

让神经网络本身符合物理的特性，比如做反对称波函数的时候，整个神经网络本身就是反对称的；但是这还是一个非常重要而且专业的问题，也是学界关注的重点.

- 其它学科的学生如何入门？

学过四大力学，学 AI 就没有压力！之后， AI 可能会像英语一样成为一个普适的工具，反而更需要交叉学科的人才涌入. 未来会有很多可能性.

- AI for Physics & Physics for AI？

现在的 AI 像炼丹或者玄学，我们之后的工作或许是理解如何设计模型，才能让 AI 的发展更加健康；但是目前人们的目光主要聚焦在如何用好 AI 的方面.

- AI 的研究需要大量数据，如何收集或者产生？

AI for Science 的应用中，有时需要数据产生器，或者通过经验优化模型，因为我们科学实验的数据是非常难以产生的.

- 能否用 AI 设计 AI 算法？

如果某一个 AI 专门被训练做这件事情，比如一个通用人工智能，很有可能实现.

## 主持人的一些小分享

其实 Hopfield 在 Ph.D 时期发的论文已经对凝聚态物理做出了很大的贡献，甚至有很多凝聚态的开创性工作，是一个非常纯正的 Theorist.
